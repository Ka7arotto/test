1 void iwl_txq_reclaim(struct iwl_trans *trans, int txq_id, int ssn,
2 		     struct sk_buff_head *skbs, bool is_flush)
4 	struct iwl_txq *txq = trans->txqs.txq[txq_id];
5 	int tfd_num, read_ptr, last_to_free;
8 	if (WARN_ON(txq_id == trans->txqs.cmd.q_id))
9 		return;
11 	if (WARN_ON(!txq))
12 		return;
14 	tfd_num = iwl_txq_get_cmd_index(txq, ssn);
15 	read_ptr = iwl_txq_get_cmd_index(txq, txq->read_ptr);
17 	spin_lock_bh(&txq->lock);
19 	if (!test_bit(txq_id, trans->txqs.queue_used)) {
20 		IWL_DEBUG_TX_QUEUES(trans, "Q %d inactive - ignoring idx %d\n",
21 				    txq_id, ssn);
22 		goto out;
25 	if (read_ptr == tfd_num)
26 		goto out;
28 	IWL_DEBUG_TX_REPLY(trans, "[Q %d] %d -> %d (%d)\n",
29 			   txq_id, txq->read_ptr, tfd_num, ssn);
33 	last_to_free = iwl_txq_dec_wrap(trans, tfd_num);
35 	if (!iwl_txq_used(txq, last_to_free)) {
36 		IWL_ERR(trans,
37 			"%s: Read index for txq id (%d), last_to_free %d is out of range [0-%d] %d %d.\n",
38 			__func__, txq_id, last_to_free,
39 			trans->trans_cfg->base_params->max_tfd_queue_size,
40 			txq->write_ptr, txq->read_ptr);
42 		iwl_op_mode_time_point(trans->op_mode,
43 				       IWL_FW_INI_TIME_POINT_FAKE_TX,
44 				       NULL);
45 		goto out;
48 	if (WARN_ON(!skb_queue_empty(skbs)))
49 		goto out;
51 	for (;
52 	     read_ptr != tfd_num;
53 	     txq->read_ptr = iwl_txq_inc_wrap(trans, txq->read_ptr),
54 	     read_ptr = iwl_txq_get_cmd_index(txq, txq->read_ptr)) {
55 		struct sk_buff *skb = txq->entries[read_ptr].skb;
57 		if (WARN_ON_ONCE(!skb))
58 			continue;
60 		iwl_txq_free_tso_page(trans, skb);
62 		__skb_queue_tail(skbs, skb);
64 		txq->entries[read_ptr].skb = NULL;
66 		if (!trans->trans_cfg->gen2)
67 			iwl_txq_gen1_inval_byte_cnt_tbl(trans, txq);
69 		iwl_txq_free_tfd(trans, txq);
72 	iwl_txq_progress(txq);
74 	if (iwl_txq_space(trans, txq) > txq->low_mark &&
75 	    test_bit(txq_id, trans->txqs.queue_stopped)) {
76 		struct sk_buff_head overflow_skbs;
77 		struct sk_buff *skb;
79 		__skb_queue_head_init(&overflow_skbs);
80 		skb_queue_splice_init(&txq->overflow_q,
81 				      is_flush ? skbs : &overflow_skbs);
90 		txq->overflow_tx = true;
99 		spin_unlock_bh(&txq->lock);
101 		while ((skb = __skb_dequeue(&overflow_skbs))) {
102 			struct iwl_device_tx_cmd *dev_cmd_ptr;
104 			dev_cmd_ptr = *(void **)((u8 *)skb->cb +
105 						 trans->txqs.dev_cmd_offs);
112 			iwl_trans_tx(trans, skb, dev_cmd_ptr, txq_id);
115 		if (iwl_txq_space(trans, txq) > txq->low_mark)
116 			iwl_wake_queue(trans, txq);
118 		spin_lock_bh(&txq->lock);
119 		txq->overflow_tx = false;
122 out:
123 	spin_unlock_bh(&txq->lock);