1 struct vm_area_struct *vma_merge(struct vma_iterator *vmi, struct mm_struct *mm,
2 			struct vm_area_struct *prev, unsigned long addr,
3 			unsigned long end, unsigned long vm_flags,
4 			struct anon_vma *anon_vma, struct file *file,
5 			pgoff_t pgoff, struct mempolicy *policy,
6 			struct vm_userfaultfd_ctx vm_userfaultfd_ctx,
7 			struct anon_vma_name *anon_name)
9 	struct vm_area_struct *curr, *next, *res;
10 	struct vm_area_struct *vma, *adjust, *remove, *remove2;
11 	struct vm_area_struct *anon_dup = NULL;
12 	struct vma_prepare vp;
13 	pgoff_t vma_pgoff;
14 	int err = 0;
15 	bool merge_prev = false;
16 	bool merge_next = false;
17 	bool vma_expanded = false;
18 	unsigned long vma_start = addr;
19 	unsigned long vma_end = end;
20 	pgoff_t pglen = (end - addr) >> PAGE_SHIFT;
21 	long adj_start = 0;
27 	if (vm_flags & VM_SPECIAL)
28 		return NULL;
31 	curr = find_vma_intersection(mm, prev ? prev->vm_end : 0, end);
33 	if (!curr ||			/* cases 1 - 4 */
34 	    end == curr->vm_end)	/* cases 6 - 8, adjacent VMA */
35 		next = vma_lookup(mm, end);
36 	else
37 		next = NULL;		/* case 5 */
39 	if (prev) {
40 		vma_start = prev->vm_start;
41 		vma_pgoff = prev->vm_pgoff;
44 		if (addr == prev->vm_end && mpol_equal(vma_policy(prev), policy)
45 		    && can_vma_merge_after(prev, vm_flags, anon_vma, file,
46 					   pgoff, vm_userfaultfd_ctx, anon_name)) {
47 			merge_prev = true;
48 			vma_prev(vmi);
53 	if (next && mpol_equal(policy, vma_policy(next)) &&
54 	    can_vma_merge_before(next, vm_flags, anon_vma, file, pgoff+pglen,
55 				 vm_userfaultfd_ctx, anon_name)) {
56 		merge_next = true;
60 	VM_WARN_ON(prev && addr <= prev->vm_start);
61 	VM_WARN_ON(curr && (addr != curr->vm_start || end > curr->vm_end));
62 	VM_WARN_ON(addr >= end);
64 	if (!merge_prev && !merge_next)
65 		return NULL; /* Not mergeable. */
67 	if (merge_prev)
68 		vma_start_write(prev);
70 	res = vma = prev;
71 	remove = remove2 = adjust = NULL;
74 	if (merge_prev && merge_next &&
75 	    is_mergeable_anon_vma(prev->anon_vma, next->anon_vma, NULL)) {
76 		vma_start_write(next);
77 		remove = next;				/* case 1 */
78 		vma_end = next->vm_end;
79 		err = dup_anon_vma(prev, next, &anon_dup);
80 		if (curr) {				/* case 6 */
81 			vma_start_write(curr);
82 			remove = curr;
83 			remove2 = next;
84 			if (!next->anon_vma)
85 				err = dup_anon_vma(prev, curr, &anon_dup);
88 		if (curr) {
89 			vma_start_write(curr);
90 			if (end == curr->vm_end) {	/* case 7 */
96 				if (curr->vm_ops && curr->vm_ops->close)
97 					err = -EINVAL;
98 				remove = curr;
100 				adjust = curr;
101 				adj_start = (end - curr->vm_start);
103 			if (!err)
104 				err = dup_anon_vma(prev, curr, &anon_dup);
107 		vma_start_write(next);
108 		res = next;
109 		if (prev && addr < prev->vm_end) {	/* case 4 */
110 			vma_start_write(prev);
111 			vma_end = addr;
112 			adjust = next;
113 			adj_start = -(prev->vm_end - addr);
114 			err = dup_anon_vma(next, prev, &anon_dup);
120 			vma = next;			/* case 3 */
121 			vma_start = addr;
122 			vma_end = next->vm_end;
123 			vma_pgoff = next->vm_pgoff - pglen;
124 			if (curr) {			/* case 8 */
125 				vma_pgoff = curr->vm_pgoff;
126 				vma_start_write(curr);
127 				remove = curr;
128 				err = dup_anon_vma(next, curr, &anon_dup);
134 	if (err)
135 		goto anon_vma_fail;
137 	if (vma_start < vma->vm_start || vma_end > vma->vm_end)
138 		vma_expanded = true;
140 	if (vma_expanded) {
141 		vma_iter_config(vmi, vma_start, vma_end);
143 		vma_iter_config(vmi, adjust->vm_start + adj_start,
144 				adjust->vm_end);
147 	if (vma_iter_prealloc(vmi, vma))
148 		goto prealloc_fail;
150 	init_multi_vma_prep(&vp, vma, adjust, remove, remove2);
151 	VM_WARN_ON(vp.anon_vma && adjust && adjust->anon_vma &&
152 		   vp.anon_vma != adjust->anon_vma);
154 	vma_prepare(&vp);
155 	vma_adjust_trans_huge(vma, vma_start, vma_end, adj_start);
157 	vma->vm_start = vma_start;
158 	vma->vm_end = vma_end;
159 	vma->vm_pgoff = vma_pgoff;
161 	if (vma_expanded)
162 		vma_iter_store(vmi, vma);
164 	if (adj_start) {
165 		adjust->vm_start += adj_start;
166 		adjust->vm_pgoff += adj_start >> PAGE_SHIFT;
167 		if (adj_start < 0) {
168 			WARN_ON(vma_expanded);
169 			vma_iter_store(vmi, next);
173 	vma_complete(&vp, vmi, mm);
174 	khugepaged_enter_vma(res, vm_flags);
175 	return res;
177 prealloc_fail:
178 	if (anon_dup)
179 		unlink_anon_vmas(anon_dup);
181 anon_vma_fail:
182 	vma_iter_set(vmi, addr);
183 	vma_iter_load(vmi);
184 	return NULL;

187 static int userfaultfd_release(struct inode *inode, struct file *file)
189 	struct userfaultfd_ctx *ctx = file->private_data;
190 	struct mm_struct *mm = ctx->mm;
191 	struct vm_area_struct *vma, *prev;
193 	struct userfaultfd_wake_range range = { .len = 0, };
194 	unsigned long new_flags;
195 	VMA_ITERATOR(vmi, mm, 0);
197 	WRITE_ONCE(ctx->released, true);
199 	if (!mmget_not_zero(mm))
200 		goto wakeup;
210 	mmap_write_lock(mm);
211 	prev = NULL;
212 	for_each_vma(vmi, vma) {
213 		cond_resched();
214 		BUG_ON(!!vma->vm_userfaultfd_ctx.ctx ^
215 		       !!(vma->vm_flags & __VM_UFFD_FLAGS));
216 		if (vma->vm_userfaultfd_ctx.ctx != ctx) {
217 			prev = vma;
218 			continue;
220 		new_flags = vma->vm_flags & ~__VM_UFFD_FLAGS;
221 		prev = vma_merge(&vmi, mm, prev, vma->vm_start, vma->vm_end,
