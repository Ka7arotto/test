1 static int xe_migrate_prepare_vm(struct xe_tile *tile, struct xe_migrate *m,
2 				 struct xe_vm *vm)
4 	struct xe_device *xe = tile_to_xe(tile);
5 	u16 pat_index = xe->pat.idx[XE_CACHE_WB];
6 	u8 id = tile->id;
7 	u32 num_entries = NUM_PT_SLOTS, num_level = vm->pt_root[id]->level;
8 	u32 map_ofs, level, i;
9 	struct xe_bo *bo, *batch = tile->mem.kernel_bb_pool->bo;
10 	u64 entry;
13 	BUILD_BUG_ON(NUM_PT_SLOTS > SZ_2M/XE_PAGE_SIZE);
15 	BUILD_BUG_ON(NUM_PT_SLOTS * XE_PAGE_SIZE % SZ_64K);
17 	BUILD_BUG_ON(!(NUM_KERNEL_PDE & 1));
20 	xe_tile_assert(tile, m->batch_base_ofs + batch->size < SZ_2M);
22 	bo = xe_bo_create_pin_map(vm->xe, tile, vm,
23 				  num_entries * XE_PAGE_SIZE,
24 				  ttm_bo_type_kernel,
25 				  XE_BO_CREATE_VRAM_IF_DGFX(tile) |
26 				  XE_BO_CREATE_PINNED_BIT);
27 	if (IS_ERR(bo))
28 		return PTR_ERR(bo);
30 	entry = vm->pt_ops->pde_encode_bo(bo, bo->size - XE_PAGE_SIZE, pat_index);
31 	xe_pt_write(xe, &vm->pt_root[id]->bo->vmap, 0, entry);
33 	map_ofs = (num_entries - num_level) * XE_PAGE_SIZE;
36 	for (i = 0, level = 0; i < num_entries; level++) {
37 		entry = vm->pt_ops->pte_encode_bo(bo, i * XE_PAGE_SIZE,
38 						  pat_index, 0);
40 		xe_map_wr(xe, &bo->vmap, map_ofs + level * 8, u64, entry);
42 		if (vm->flags & XE_VM_FLAG_64K)
43 			i += 16;
44 		else
45 			i += 1;
48 	if (!IS_DGFX(xe)) {
50 		m->batch_base_ofs = NUM_PT_SLOTS * XE_PAGE_SIZE;
51 		for (i = 0; i < batch->size;
52 		     i += vm->flags & XE_VM_FLAG_64K ? XE_64K_PAGE_SIZE :
53 		     XE_PAGE_SIZE) {
54 			entry = vm->pt_ops->pte_encode_bo(batch, i,
55 							  pat_index, 0);
57 			xe_map_wr(xe, &bo->vmap, map_ofs + level * 8, u64,
58 				  entry);
59 			level++;
61 		if (xe->info.has_usm) {
62 			xe_tile_assert(tile, batch->size == SZ_1M);
64 			batch = tile->primary_gt->usm.bb_pool->bo;
65 			m->usm_batch_base_ofs = m->batch_base_ofs + SZ_1M;
66 			xe_tile_assert(tile, batch->size == SZ_512K);
68 			for (i = 0; i < batch->size;
69 			     i += vm->flags & XE_VM_FLAG_64K ? XE_64K_PAGE_SIZE :
70 			     XE_PAGE_SIZE) {
71 				entry = vm->pt_ops->pte_encode_bo(batch, i,
72 								  pat_index, 0);
74 				xe_map_wr(xe, &bo->vmap, map_ofs + level * 8, u64,
75 					  entry);
76 				level++;
80 		u64 batch_addr = xe_bo_addr(batch, 0, XE_PAGE_SIZE);
82 		m->batch_base_ofs = xe_migrate_vram_ofs(xe, batch_addr);
84 		if (xe->info.has_usm) {
85 			batch = tile->primary_gt->usm.bb_pool->bo;
86 			batch_addr = xe_bo_addr(batch, 0, XE_PAGE_SIZE);
87 			m->usm_batch_base_ofs = xe_migrate_vram_ofs(xe, batch_addr);
91 	for (level = 1; level < num_level; level++) {
92 		u32 flags = 0;
94 		if (vm->flags & XE_VM_FLAG_64K && level == 1)
95 			flags = XE_PDE_64K;
97 		entry = vm->pt_ops->pde_encode_bo(bo, map_ofs + (level - 1) *
98 						  XE_PAGE_SIZE, pat_index);
99 		xe_map_wr(xe, &bo->vmap, map_ofs + XE_PAGE_SIZE * level, u64,
100 			  entry | flags);
104 	for (i = 0; i < num_entries - num_level; i++) {
105 		entry = vm->pt_ops->pde_encode_bo(bo, i * XE_PAGE_SIZE,
106 						  pat_index);
108 		xe_map_wr(xe, &bo->vmap, map_ofs + XE_PAGE_SIZE +
109 			  (i + 1) * 8, u64, entry);
113 	level = 2;
114 	xe_map_wr(xe, &bo->vmap, map_ofs + XE_PAGE_SIZE * level + 255 * 8, u64,
115 		  vm->pt_ops->pte_encode_addr(xe, 0, pat_index, level, IS_DGFX(xe), 0)
116 		  | XE_PTE_NULL);
117 	m->cleared_mem_ofs = (255ULL << xe_pt_shift(level));
120 	if (IS_DGFX(xe)) {
121 		u64 pos, ofs, flags;
123 		level = 2;
124 		ofs = map_ofs + XE_PAGE_SIZE * level + 256 * 8;
125 		flags = vm->pt_ops->pte_encode_addr(xe, 0, pat_index, level,
126 						    true, 0);
132 		for (pos = xe->mem.vram.dpa_base;
133 		     pos < xe->mem.vram.actual_physical_size + xe->mem.vram.dpa_base;
134 		     pos += SZ_1G, ofs += 8)
135 			xe_map_wr(xe, &bo->vmap, ofs, u64, pos | flags);
157 #define NUM_VMUSA_UNIT_PER_PAGE	32
158 #define VM_SA_UPDATE_UNIT_SIZE		(XE_PAGE_SIZE / NUM_VMUSA_UNIT_PER_PAGE)
159 #define NUM_VMUSA_WRITES_PER_UNIT	(VM_SA_UPDATE_UNIT_SIZE / sizeof(u64))
160 	drm_suballoc_manager_init(&m->vm_update_sa,