Q: Given the following code slice:
```
1 int spi_sync(struct spi_device *spi, struct spi_message *message)
2 {
3 	int ret;
4 
5 	mutex_lock(&spi->controller->bus_lock_mutex);
6 	ret = __spi_sync(spi, message);
7 	mutex_unlock(&spi->controller->bus_lock_mutex);
8 
9 	return ret;
10 }
11 static inline void might_sleep(void)
12 {
13 	BUG_ON(preempt_disable_count);
14 }
15 void __sched mutex_lock(struct mutex *lock)
16 {
17 	might_sleep();
18 
19 	if (!__mutex_trylock_fast(lock))
20 		__mutex_lock_slowpath(lock);
21 }
22 static long __sched
23 wait_for_common(struct completion *x, long timeout, int state)
24 {
25 	return __wait_for_common(x, schedule_timeout, timeout, state);
26 }
27 void __sched wait_for_completion(struct completion *x)
28 {
29 	wait_for_common(x, MAX_SCHEDULE_TIMEOUT, TASK_UNINTERRUPTIBLE);
30 }
31 static int __spi_sync(struct spi_device *spi, struct spi_message *message)
32 {
33 	DECLARE_COMPLETION_ONSTACK(done);
34 	int status;
35 	struct spi_controller *ctlr = spi->controller;
36 
37 	if (__spi_check_suspended(ctlr)) {
38 		dev_warn_once(&spi->dev, "Attempted to sync while suspend\n");
39 		return -ESHUTDOWN;
40 	}
41 
42 	status = __spi_validate(spi, message);
43 	if (status != 0)
44 		return status;
45 
46 	message->spi = spi;
47 
48 	SPI_STATISTICS_INCREMENT_FIELD(ctlr->pcpu_statistics, spi_sync);
49 	SPI_STATISTICS_INCREMENT_FIELD(spi->pcpu_statistics, spi_sync);
50 
51 	/*
52 	 * Checking queue_empty here only guarantees async/sync message
53 	 * ordering when coming from the same context. It does not need to
54 	 * guard against reentrancy from a different context. The io_mutex
55 	 * will catch those cases.
56 	 */
57 	if (READ_ONCE(ctlr->queue_empty) && !ctlr->must_async) {
58 		message->actual_length = 0;
59 		message->status = -EINPROGRESS;
60 
61 		trace_spi_message_submit(message);
62 
63 		SPI_STATISTICS_INCREMENT_FIELD(ctlr->pcpu_statistics, spi_sync_immediate);
64 		SPI_STATISTICS_INCREMENT_FIELD(spi->pcpu_statistics, spi_sync_immediate);
65 
66 		__spi_transfer_message_noqueue(ctlr, message);
67 
68 		return message->status;
69 	}
70 
71 	/*
72 	 * There are messages in the async queue that could have originated
73 	 * from the same context, so we need to preserve ordering.
74 	 * Therefor we send the message to the async queue and wait until they
75 	 * are completed.
76 	 */
77 	message->complete = spi_complete;
78 	message->context = &done;
79 	status = spi_async_locked(spi, message);
80 	if (status == 0) {
81 		wait_for_completion(&done);
82 		status = message->status;
83 	}
84 	message->complete = NULL;
85 	message->context = NULL;
86 
87 	return status;
88 }
89 static inline int list_is_last(const struct list_head *list,
90 				const struct list_head *head)
91 {
92 	return list->next == head;
93 }
94 void __sched mutex_unlock(struct mutex *lock)
95 {
96 	mutex_release(&lock->dep_map, _RET_IP_);
97 	__rt_mutex_unlock(&lock->rtmutex);
98 }
99 int spi_split_transfers_maxsize(struct spi_controller *ctlr,
100 				struct spi_message *msg,
101 				size_t maxsize,
102 				gfp_t gfp)
103 {
104 	struct spi_transfer *xfer;
105 	int ret;
106 
107 	/*
108 	 * Iterate over the transfer_list,
109 	 * but note that xfer is advanced to the last transfer inserted
110 	 * to avoid checking sizes again unnecessarily (also xfer does
111 	 * potentially belong to a different list by the time the
112 	 * replacement has happened).
113 	 */
114 	list_for_each_entry(xfer, &msg->transfers, transfer_list) {
115 		if (xfer->len > maxsize) {
116 			ret = __spi_split_transfer_maxsize(ctlr, msg, &xfer,
117 							   maxsize, gfp);
118 			if (ret)
119 				return ret;
120 		}
121 	}
122 
123 	return 0;
124 }
125 static int __spi_pump_transfer_message(struct spi_controller *ctlr,
126 		struct spi_message *msg, bool was_busy)
127 {
128 	struct spi_transfer *xfer;
129 	int ret;
130 
131 	if (!was_busy && ctlr->auto_runtime_pm) {
132 		ret = pm_runtime_get_sync(ctlr->dev.parent);
133 		if (ret < 0) {
134 			pm_runtime_put_noidle(ctlr->dev.parent);
135 			dev_err(&ctlr->dev, "Failed to power device: %d\n",
136 				ret);
137 
138 			msg->status = ret;
139 			spi_finalize_current_message(ctlr);
140 
141 			return ret;
142 		}
143 	}
144 
145 	if (!was_busy)
146 		trace_spi_controller_busy(ctlr);
147 
148 	if (!was_busy && ctlr->prepare_transfer_hardware) {
149 		ret = ctlr->prepare_transfer_hardware(ctlr);
150 		if (ret) {
151 			dev_err(&ctlr->dev,
152 				"failed to prepare transfer hardware: %d\n",
153 				ret);
154 
155 			if (ctlr->auto_runtime_pm)
156 				pm_runtime_put(ctlr->dev.parent);
157 
158 			msg->status = ret;
159 			spi_finalize_current_message(ctlr);
160 
161 			return ret;
162 		}
163 	}
164 
165 	trace_spi_message_start(msg);
166 
167 	ret = spi_split_transfers_maxsize(ctlr, msg,
168 					  spi_max_transfer_size(msg->spi),
169 					  GFP_KERNEL | GFP_DMA);
170 	if (ret) {
171 		msg->status = ret;
172 		spi_finalize_current_message(ctlr);
173 		return ret;
174 	}
175 
176 	if (ctlr->prepare_message) {
177 		ret = ctlr->prepare_message(ctlr, msg);
178 		if (ret) {
179 			dev_err(&ctlr->dev, "failed to prepare message: %d\n",
180 				ret);
181 			msg->status = ret;
182 			spi_finalize_current_message(ctlr);
183 			return ret;
184 		}
185 		msg->prepared = true;
186 	}
187 
188 	ret = spi_map_msg(ctlr, msg);
189 	if (ret) {
190 		msg->status = ret;
191 		spi_finalize_current_message(ctlr);
192 		return ret;
193 	}
194 
195 	if (!ctlr->ptp_sts_supported && !ctlr->transfer_one) {
196 		list_for_each_entry(xfer, &msg->transfers, transfer_list) {
197 			xfer->ptp_sts_word_pre = 0;
198 			ptp_read_system_prets(xfer->ptp_sts);
199 		}
200 	}
201 
202 	/*
203 	 * Drivers implementation of transfer_one_message() must arrange for
204 	 * spi_finalize_current_message() to get called. Most drivers will do
205 	 * this in the calling context, but some don't. For those cases, a
206 	 * completion is used to guarantee that this function does not return
207 	 * until spi_finalize_current_message() is done accessing
208 	 * ctlr->cur_msg.
209 	 * Use of the following two flags enable to opportunistically skip the
210 	 * use of the completion since its use involves expensive spin locks.
211 	 * In case of a race with the context that calls
212 	 * spi_finalize_current_message() the completion will always be used,
213 	 * due to strict ordering of these flags using barriers.
214 	 */
215 	WRITE_ONCE(ctlr->cur_msg_incomplete, true);
216 	WRITE_ONCE(ctlr->cur_msg_need_completion, false);
217 	reinit_completion(&ctlr->cur_msg_completion);
218 	smp_wmb(); /* Make these available to spi_finalize_current_message() */
219 
220 	ret = ctlr->transfer_one_message(ctlr, msg);
221 	if (ret) {
222 		dev_err(&ctlr->dev,
223 			"failed to transfer one message from queue\n");
224 		return ret;
225 	}
226 
227 	WRITE_ONCE(ctlr->cur_msg_need_completion, true);
228 	smp_mb(); /* See spi_finalize_current_message()... */
229 	if (READ_ONCE(ctlr->cur_msg_incomplete))
230 		wait_for_completion(&ctlr->cur_msg_completion);
231 
232 	return 0;
233 }
234 static void __spi_transfer_message_noqueue(struct spi_controller *ctlr, struct spi_message *msg)
235 {
236 	bool was_busy;
237 	int ret;
238 
239 	mutex_lock(&ctlr->io_mutex);
240 
241 	was_busy = ctlr->busy;
242 
243 	ctlr->cur_msg = msg;
244 	ret = __spi_pump_transfer_message(ctlr, msg, was_busy);
245 	if (ret)
246 		dev_err(&ctlr->dev, "noqueue transfer failed\n");
247 	ctlr->cur_msg = NULL;
248 	ctlr->fallback = false;
249 
250 	if (!was_busy) {
251 		kfree(ctlr->dummy_rx);
252 		ctlr->dummy_rx = NULL;
253 		kfree(ctlr->dummy_tx);
254 		ctlr->dummy_tx = NULL;
255 		if (ctlr->unprepare_transfer_hardware &&
256 		    ctlr->unprepare_transfer_hardware(ctlr))
257 			dev_err(&ctlr->dev,
258 				"failed to unprepare transfer hardware\n");
259 		spi_idle_runtime_pm(ctlr);
260 	}
261 
262 	mutex_unlock(&ctlr->io_mutex);
263 }
264 static inline int __spi_check_suspended(const struct spi_controller *ctlr)
265 {
266 	return ctlr->flags & SPI_CONTROLLER_SUSPENDED ? -ESHUTDOWN : 0;
267 }
268 static __always_inline void __rt_mutex_unlock(struct rt_mutex_base *lock)
269 {
270 	if (likely(rt_mutex_cmpxchg_release(lock, current, NULL)))
271 		return;
272 
273 	rt_mutex_slowunlock(lock);
274 }
275 static __always_inline bool
276 atomic_long_try_cmpxchg_acquire(atomic_long_t *v, long *old, long new)
277 {
278 	instrument_atomic_read_write(v, sizeof(*v));
279 	instrument_atomic_read_write(old, sizeof(*old));
280 	return arch_atomic_long_try_cmpxchg_acquire(v, old, new);
281 }
282 static int __spi_async(struct spi_device *spi, struct spi_message *message)
283 {
284 	struct spi_controller *ctlr = spi->controller;
285 	struct spi_transfer *xfer;
286 
287 	/*
288 	 * Some controllers do not support doing regular SPI transfers. Return
289 	 * ENOTSUPP when this is the case.
290 	 */
291 	if (!ctlr->transfer)
292 		return -ENOTSUPP;
293 
294 	message->spi = spi;
295 
296 	SPI_STATISTICS_INCREMENT_FIELD(ctlr->pcpu_statistics, spi_async);
297 	SPI_STATISTICS_INCREMENT_FIELD(spi->pcpu_statistics, spi_async);
298 
299 	trace_spi_message_submit(message);
300 
301 	if (!ctlr->ptp_sts_supported) {
302 		list_for_each_entry(xfer, &message->transfers, transfer_list) {
303 			xfer->ptp_sts_word_pre = 0;
304 			ptp_read_system_prets(xfer->ptp_sts);
305 		}
306 	}
307 
308 	return ctlr->transfer(spi, message);
309 }
310 static inline void kfree(void *p)
311 {
312 	if (p >= __kfree_ignore_start && p < __kfree_ignore_end)
313 		return;
314 	free(p);
315 }
316 static void __sched rt_mutex_slowunlock(struct rt_mutex_base *lock)
317 {
318 	DEFINE_RT_WAKE_Q(wqh);
319 	unsigned long flags;
320 
321 	/* irqsave required to support early boot calls */
322 	raw_spin_lock_irqsave(&lock->wait_lock, flags);
323 
324 	debug_rt_mutex_unlock(lock);
325 
326 	/*
327 	 * We must be careful here if the fast path is enabled. If we
328 	 * have no waiters queued we cannot set owner to NULL here
329 	 * because of:
330 	 *
331 	 * foo->lock->owner = NULL;
332 	 *			rtmutex_lock(foo->lock);   <- fast path
333 	 *			free = atomic_dec_and_test(foo->refcnt);
334 	 *			rtmutex_unlock(foo->lock); <- fast path
335 	 *			if (free)
336 	 *				kfree(foo);
337 	 * raw_spin_unlock(foo->lock->wait_lock);
338 	 *
339 	 * So for the fastpath enabled kernel:
340 	 *
341 	 * Nothing can set the waiters bit as long as we hold
342 	 * lock->wait_lock. So we do the following sequence:
343 	 *
344 	 *	owner = rt_mutex_owner(lock);
345 	 *	clear_rt_mutex_waiters(lock);
346 	 *	raw_spin_unlock(&lock->wait_lock);
347 	 *	if (cmpxchg(&lock->owner, owner, 0) == owner)
348 	 *		return;
349 	 *	goto retry;
350 	 *
351 	 * The fastpath disabled variant is simple as all access to
352 	 * lock->owner is serialized by lock->wait_lock:
353 	 *
354 	 *	lock->owner = NULL;
355 	 *	raw_spin_unlock(&lock->wait_lock);
356 	 */
357 	while (!rt_mutex_has_waiters(lock)) {
358 		/* Drops lock->wait_lock ! */
359 		if (unlock_rt_mutex_safe(lock, flags) == true)
360 			return;
361 		/* Relock the rtmutex and try again */
362 		raw_spin_lock_irqsave(&lock->wait_lock, flags);
363 	}
364 
365 	/*
366 	 * The wakeup next waiter path does not suffer from the above
367 	 * race. See the comments there.
368 	 *
369 	 * Queue the next waiter for wakeup once we release the wait_lock.
370 	 */
371 	mark_wakeup_next_waiter(&wqh, lock);
372 	raw_spin_unlock_irqrestore(&lock->wait_lock, flags);
373 
374 	rt_mutex_wake_up_q(&wqh);
375 }
376 static void spi_idle_runtime_pm(struct spi_controller *ctlr)
377 {
378 	if (ctlr->auto_runtime_pm) {
379 		pm_runtime_mark_last_busy(ctlr->dev.parent);
380 		pm_runtime_put_autosuspend(ctlr->dev.parent);
381 	}
382 }
383 static inline void spin_unlock_irqrestore(spinlock_t *lock, unsigned long f)
384 {
385 	spin_unlock(lock);
386 }
387 static noinline void __sched
388 __mutex_lock_slowpath(struct mutex *lock)
389 {
390 	__mutex_lock(lock, TASK_UNINTERRUPTIBLE, 0, NULL, _RET_IP_);
391 }
392 static int __spi_validate_bits_per_word(struct spi_controller *ctlr,
393 					u8 bits_per_word)
394 {
395 	if (ctlr->bits_per_word_mask) {
396 		/* Only 32 bits fit in the mask */
397 		if (bits_per_word > 32)
398 			return -EINVAL;
399 		if (!(ctlr->bits_per_word_mask & SPI_BPW_MASK(bits_per_word)))
400 			return -EINVAL;
401 	}
402 
403 	return 0;
404 }
405 static inline void spin_lock_irqsave(spinlock_t *lock, unsigned long f)
406 {
407 	spin_lock(lock);
408 }
409 static int __spi_validate(struct spi_device *spi, struct spi_message *message)
410 {
411 	struct spi_controller *ctlr = spi->controller;
412 	struct spi_transfer *xfer;
413 	int w_size;
414 
415 	if (list_empty(&message->transfers))
416 		return -EINVAL;
417 
418 	/*
419 	 * If an SPI controller does not support toggling the CS line on each
420 	 * transfer (indicated by the SPI_CS_WORD flag) or we are using a GPIO
421 	 * for the CS line, we can emulate the CS-per-word hardware function by
422 	 * splitting transfers into one-word transfers and ensuring that
423 	 * cs_change is set for each transfer.
424 	 */
425 	if ((spi->mode & SPI_CS_WORD) && (!(ctlr->mode_bits & SPI_CS_WORD) ||
426 					  spi_get_csgpiod(spi, 0))) {
427 		size_t maxsize;
428 		int ret;
429 
430 		maxsize = (spi->bits_per_word + 7) / 8;
431 
432 		/* spi_split_transfers_maxsize() requires message->spi */
433 		message->spi = spi;
434 
435 		ret = spi_split_transfers_maxsize(ctlr, message, maxsize,
436 						  GFP_KERNEL);
437 		if (ret)
438 			return ret;
439 
440 		list_for_each_entry(xfer, &message->transfers, transfer_list) {
441 			/* Don't change cs_change on the last entry in the list */
442 			if (list_is_last(&xfer->transfer_list, &message->transfers))
443 				break;
444 			xfer->cs_change = 1;
445 		}
446 	}
447 
448 	/*
449 	 * Half-duplex links include original MicroWire, and ones with
450 	 * only one data pin like SPI_3WIRE (switches direction) or where
451 	 * either MOSI or MISO is missing.  They can also be caused by
452 	 * software limitations.
453 	 */
454 	if ((ctlr->flags & SPI_CONTROLLER_HALF_DUPLEX) ||
455 	    (spi->mode & SPI_3WIRE)) {
456 		unsigned flags = ctlr->flags;
457 
458 		list_for_each_entry(xfer, &message->transfers, transfer_list) {
459 			if (xfer->rx_buf && xfer->tx_buf)
460 				return -EINVAL;
461 			if ((flags & SPI_CONTROLLER_NO_TX) && xfer->tx_buf)
462 				return -EINVAL;
463 			if ((flags & SPI_CONTROLLER_NO_RX) && xfer->rx_buf)
464 				return -EINVAL;
465 		}
466 	}
467 
468 	/*
469 	 * Set transfer bits_per_word and max speed as spi device default if
470 	 * it is not set for this transfer.
471 	 * Set transfer tx_nbits and rx_nbits as single transfer default
472 	 * (SPI_NBITS_SINGLE) if it is not set for this transfer.
473 	 * Ensure transfer word_delay is at least as long as that required by
474 	 * device itself.
475 	 */
476 	message->frame_length = 0;
477 	list_for_each_entry(xfer, &message->transfers, transfer_list) {
478 		xfer->effective_speed_hz = 0;
479 		message->frame_length += xfer->len;
480 		if (!xfer->bits_per_word)
481 			xfer->bits_per_word = spi->bits_per_word;
482 
483 		if (!xfer->speed_hz)
484 			xfer->speed_hz = spi->max_speed_hz;
485 
486 		if (ctlr->max_speed_hz && xfer->speed_hz > ctlr->max_speed_hz)
487 			xfer->speed_hz = ctlr->max_speed_hz;
488 
489 		if (__spi_validate_bits_per_word(ctlr, xfer->bits_per_word))
490 			return -EINVAL;
491 
492 		/*
493 		 * SPI transfer length should be multiple of SPI word size
494 		 * where SPI word size should be power-of-two multiple.
495 		 */
496 		if (xfer->bits_per_word <= 8)
497 			w_size = 1;
498 		else if (xfer->bits_per_word <= 16)
499 			w_size = 2;
500 		else
501 			w_size = 4;
502 
503 		/* No partial transfers accepted */
504 		if (xfer->len % w_size)
505 			return -EINVAL;
506 
507 		if (xfer->speed_hz && ctlr->min_speed_hz &&
508 		    xfer->speed_hz < ctlr->min_speed_hz)
509 			return -EINVAL;
510 
511 		if (xfer->tx_buf && !xfer->tx_nbits)
512 			xfer->tx_nbits = SPI_NBITS_SINGLE;
513 		if (xfer->rx_buf && !xfer->rx_nbits)
514 			xfer->rx_nbits = SPI_NBITS_SINGLE;
515 		/*
516 		 * Check transfer tx/rx_nbits:
517 		 * 1. check the value matches one of single, dual and quad
518 		 * 2. check tx/rx_nbits match the mode in spi_device
519 		 */
520 		if (xfer->tx_buf) {
521 			if (spi->mode & SPI_NO_TX)
522 				return -EINVAL;
523 			if (xfer->tx_nbits != SPI_NBITS_SINGLE &&
524 				xfer->tx_nbits != SPI_NBITS_DUAL &&
525 				xfer->tx_nbits != SPI_NBITS_QUAD)
526 				return -EINVAL;
527 			if ((xfer->tx_nbits == SPI_NBITS_DUAL) &&
528 				!(spi->mode & (SPI_TX_DUAL | SPI_TX_QUAD)))
529 				return -EINVAL;
530 			if ((xfer->tx_nbits == SPI_NBITS_QUAD) &&
531 				!(spi->mode & SPI_TX_QUAD))
532 				return -EINVAL;
533 		}
534 		/* Check transfer rx_nbits */
535 		if (xfer->rx_buf) {
536 			if (spi->mode & SPI_NO_RX)
537 				return -EINVAL;
538 			if (xfer->rx_nbits != SPI_NBITS_SINGLE &&
539 				xfer->rx_nbits != SPI_NBITS_DUAL &&
540 				xfer->rx_nbits != SPI_NBITS_QUAD)
541 				return -EINVAL;
542 			if ((xfer->rx_nbits == SPI_NBITS_DUAL) &&
543 				!(spi->mode & (SPI_RX_DUAL | SPI_RX_QUAD)))
544 				return -EINVAL;
545 			if ((xfer->rx_nbits == SPI_NBITS_QUAD) &&
546 				!(spi->mode & SPI_RX_QUAD))
547 				return -EINVAL;
548 		}
549 
550 		if (_spi_xfer_word_delay_update(xfer, spi))
551 			return -EINVAL;
552 	}
553 
554 	message->status = -EINPROGRESS;
555 
556 	return 0;
557 }
558 static int spi_async_locked(struct spi_device *spi, struct spi_message *message)
559 {
560 	struct spi_controller *ctlr = spi->controller;
561 	int ret;
562 	unsigned long flags;
563 
564 	ret = __spi_validate(spi, message);
565 	if (ret != 0)
566 		return ret;
567 
568 	spin_lock_irqsave(&ctlr->bus_lock_spinlock, flags);
569 
570 	ret = __spi_async(spi, message);
571 
572 	spin_unlock_irqrestore(&ctlr->bus_lock_spinlock, flags);
573 
574 	return ret;
575 
576 }
577 static __always_inline bool rt_mutex_cmpxchg_release(struct rt_mutex_base *lock,
578 						     struct task_struct *old,
579 						     struct task_struct *new)
580 {
581 	return try_cmpxchg_release(&lock->owner, &old, new);
582 }
583 static __always_inline bool __mutex_trylock_fast(struct mutex *lock)
584 {
585 	unsigned long curr = (unsigned long)current;
586 	unsigned long zero = 0UL;
587 
588 	if (atomic_long_try_cmpxchg_acquire(&lock->owner, &zero, curr))
589 		return true;
590 
591 	return false;
592 }
593 static inline struct gpio_desc *spi_get_csgpiod(const struct spi_device *spi, u8 idx)
594 {
595 	return spi->cs_gpiod;
596 }
597 static int _spi_xfer_word_delay_update(struct spi_transfer *xfer,
598 				       struct spi_device *spi)
599 {
600 	int delay1, delay2;
601 
602 	delay1 = spi_delay_to_ns(&xfer->word_delay, xfer);
603 	if (delay1 < 0)
604 		return delay1;
605 
606 	delay2 = spi_delay_to_ns(&spi->word_delay, xfer);
607 	if (delay2 < 0)
608 		return delay2;
609 
610 	if (delay1 < delay2)
611 		memcpy(&xfer->word_delay, &spi->word_delay,
612 		       sizeof(xfer->word_delay));
613 
614 	return 0;
615 }
616 static int __sched
617 __mutex_lock(struct mutex *lock, unsigned int state, unsigned int subclass,
618 	     struct lockdep_map *nest_lock, unsigned long ip)
619 {
620 	return __mutex_lock_common(lock, state, subclass, nest_lock, ip, NULL, false);
621 }
622 static inline int list_empty(const struct list_head *head)
623 {
624 	return head->next == head;
625 }
```
which has a CWE-457 vulnerability at line:
```
7 	mutex_unlock(&spi->controller->bus_lock_mutex);
```
Please generate five possible patches for the vulnerability.